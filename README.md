# Comment Toxicity Detector ðŸ’¬ðŸš«

The Comment Toxicity Detector is a machine learning project aimed at identifying toxic comments in text. It uses natural language processing (NLP) techniques to classify comments as toxic or non-toxic, helping to automate the moderation of online communities.

# ðŸš€ Key Features

	â€¢	Toxicity Classification: Detects toxic language in comments, including categories like insult, threat, and hate speech.
	â€¢	End-to-End NLP Pipeline: From data preprocessing to model training and prediction.
	â€¢	Pretrained Model: Efficiently classifies comments using a trained model.
	â€¢	Dataset: Based on the Jigsaw Toxic Comment Classification Challenge, which contains a large set of labeled comments.
    link: https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data

# ðŸ§  How It Works

The project uses the Jigsaw Toxic Comment Classification Challenge dataset. The model is designed to:

	1.	Data Preprocessing:
	â€¢	Tokenizes and vectorizes comments for training and testing.
	â€¢	Cleans text to remove unnecessary characters and stopwords.
	2.	Model Training:
	â€¢	The model is trained using multiple labels to classify comments as toxic, severe toxic, obscene, threat, insult, or identity hate.
	3.	Prediction:
	â€¢	After training, the model predicts the toxicity of new comments and outputs the corresponding class labels.

# ðŸ”§ Usage

	1.	Jupyter Notebook:
	â€¢	Run the CommentToxicity.ipynb to load the dataset, train the model, and classify new comments.
